import os
import torch
import scipy
from transformers import pipeline
from datetime import datetime

# ================= é…ç½®åŒº =================
# å¯é€‰æ¨¡å‹: 'facebook/musicgen-small', 'facebook/musicgen-medium', 'facebook/musicgen-large'
# M4 16GB æ¨è: 'facebook/musicgen-small' (æœ€å¿«) æˆ– 'facebook/musicgen-medium'
MODEL_NAME = 'facebook/musicgen-small'
OUTPUT_DIR = "assets/music_candidates_local"

# ç”Ÿæˆæ—¶é•¿æ§åˆ¶
# MusicGen çº¦ 50 tokens/ç§’ (è¿™ä¸ªæ•°å€¼ä¸ä¸€å®šç²¾ç¡®ï¼Œå–å†³äºæ¨¡å‹é…ç½®)
# small/medium æ¨¡å‹é»˜è®¤æœ€å¤§é•¿åº¦é€šå¸¸é™åˆ¶åœ¨ 30ç§’å·¦å³ (1500 tokens)
DURATION_SECONDS = 30
MAX_NEW_TOKENS = int(DURATION_SECONDS * 51.2) # ç»éªŒå€¼: 1500 tokens ~= 30s

# å®šä¹‰éŸ³ä¹é£æ ¼ Prompt
STYLES = {
    "playful_chinese": [
        "Background music for kids education video, traditional chinese instruments, guzheng and dizi flute, blended with light lo-fi hip hop beat, playful, happy, bright, medium tempo"
    ],
    "smart_thinking": [
        "Background music, pizzicato strings, light percussion, marimba, curious mood, thinking process, puzzle solving, subtle asian melody, clean, minimal"
    ],
    "epic_wisdom": [
        "Cinematic underscore, inspiring, orchestral, chinese drums and strings, victory, wisdom, ancient china atmosphere but modern production, uplifting"
    ]
}
# =========================================

def generate_music_local():
    """
    åœ¨æœ¬åœ°ä½¿ç”¨ Hugging Face Transformers ç”ŸæˆéŸ³ä¹
    """
    print(f"ğŸ–¥ï¸  æ­£åœ¨æ£€æŸ¥ç¡¬ä»¶ç¯å¢ƒ...")
    
    # æ£€æŸ¥è®¾å¤‡
    if torch.backends.mps.is_available():
        device = "mps"
        print("ğŸš€ æ£€æµ‹åˆ° Apple Silicon (MPS) åŠ é€Ÿå¼€å¯")
    elif torch.cuda.is_available():
        device = "cuda"
        print("ğŸš€ æ£€æµ‹åˆ° NVIDIA GPU (CUDA) åŠ é€Ÿå¼€å¯")
    else:
        device = "cpu"
        print("ğŸ¢ æœªæ£€æµ‹åˆ° GPUï¼Œä½¿ç”¨ CPU è¿è¡Œ (ä¼šæ¯”è¾ƒæ…¢)")

    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)

    print(f"â³ æ­£åœ¨åŠ è½½æ¨¡å‹: {MODEL_NAME} (ç¬¬ä¸€æ¬¡è¿è¡Œä¼šä¸‹è½½çº¦ 1-2GB)...")
    
    # åˆå§‹åŒ– Pipeline
    # æ³¨æ„: mps åœ¨æŸäº› transformer ç‰ˆæœ¬ä¸‹å¯èƒ½ä¸ç¨³å®šï¼Œå¦‚æœæŠ¥é”™è¯·æ”¹ä¸º device="cpu"
    try:
        synthesiser = pipeline("text-to-audio", model=MODEL_NAME, device=device)
    except Exception as e:
        print(f"âš ï¸  åŠ è½½è®¾å¤‡ {device} å¤±è´¥ï¼Œå°è¯•åˆ‡æ¢å› CPU... ({e})")
        synthesiser = pipeline("text-to-audio", model=MODEL_NAME, device="cpu")

    print(f"ğŸ¹ å¼€å§‹ç”Ÿæˆï¼Œå…± {len(STYLES)} ç§é£æ ¼...")

    for style_name, prompts in STYLES.items():
        for i, prompt in enumerate(prompts):
            print(f"\nğŸ¼ æ­£åœ¨ç”Ÿæˆé£æ ¼: [{style_name}] ({i+1}/{len(prompts)})")
            print(f"   Prompt: {prompt[:50]}...")
            
            # ç”Ÿæˆ
            # forward_params æ§åˆ¶ç”Ÿæˆå‚æ•°
            music = synthesiser(
                prompt, 
                forward_params={
                    "max_new_tokens": MAX_NEW_TOKENS,
                    "do_sample": True
                }
            )
            
            # music æ ¼å¼: {'audio': np.array(shape=(1, N)), 'sampling_rate': 32000}
            audio_data = music['audio'][0] # å–å‡ºå•å£°é“/ç¬¬ä¸€è½¨ (é€šå¸¸æ˜¯ (C, T) æˆ– (T,))
            # å¦‚æœæ˜¯ (C, T) ä¸” C=1, scipy éœ€è¦ (T,) æˆ– (T, C)
            if audio_data.ndim == 2 and audio_data.shape[0] == 1:
                audio_data = audio_data.squeeze(0)
                
            sample_rate = music['sampling_rate']

            # ä¿å­˜æ–‡ä»¶
            timestamp = datetime.now().strftime('%H%M')
            filename = f"{style_name}_{timestamp}.wav"
            filepath = os.path.join(OUTPUT_DIR, filename)
            
            scipy.io.wavfile.write(filepath, rate=sample_rate, data=audio_data)
            print(f"   âœ… å·²ä¿å­˜: {filepath}")

    print(f"\nğŸ‰ å…¨éƒ¨å®Œæˆï¼è¯·æ£€æŸ¥ç›®å½•: {OUTPUT_DIR}")

if __name__ == "__main__":
    generate_music_local()
